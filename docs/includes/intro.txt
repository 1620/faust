**Faust** is a stream processing library, bringing the groundbreaking
new ideas from `Kafka Streams`_ into Python.

It is used at `Robinhood`_ to build high performance distributed systems
and real-time data pipelines that process billions of events every day.

It's a powerful tool you can drop into any Python program
to easily build traditionally complicated distributed systems
that are high performance and fault tolerant.

Faust provides both *stream processing* and *event processing*,
sharing similarity with tools such as `Celery`_,
`Kafka Streams`_, `Apache Spark`_/`Storm`_/`Samza`_, and `Flink`_.

It does not use a DSL, instead it uses modern Python features
to blend into normal Python code so that you can use existing
tools such as NumPy, PyTorch, Pandas, NLTK, Django, Flask, SQLAlchemy,
and all the other tools that you like in Python.

Faust takes advantage of the new `async/await`_ syntax added recently
to Python, and so requires Python 3.6 or later.

Here's an example agent processing "order events":

.. sourcecode:: python

    app = faust.App('myapp', broker='kafka://localhost')

    # Models describe how messages are serialized:
    # {"account_id": "3fae-...", amount": 3}
    class Order(faust.Record):
        account_id: str
        amount: int

    @app.agent(value_type=Order)
    async def order(orders):
        async for order in orders:
            print(f'Order for {order.account_id}: {order.amount}')
            # do something with order

The Agent is a stream processor that can execute on
many machines and CPU cores. You can think of it as similar
to a Celery task, but radically different in that it can keep
state between executing tasks.

State can be in-memory, or it can be stored in "tables",
which are named distributed key/value stores you can use as
a regular Python dictionary.

Tables are partitioned by key and stored locally using `RocksDB`,
a superfast embedded database library written in C++.

For reliability we use a Kafka topic as "write-ahead-log".
Whenever a key is changed we publish to the changelog,
and standy nodes consume from this changelog to keep an exact replica
of the data for instant recovery should any of the nodes fail.

To the user a table is just a dictionary, for example you
could count page views by URLs like this:

.. sourcecode:: python

    # data sent to 'clicks' topic with key="http://example.com" value="1"
    click_topic = app.topic('clicks', key_type=str, value_type=str)

    # default value for missing URL will be 0 with `default=int`
    counts = app.Table('click_counts', default=int)

    @app.agent(click_topic)
    async def count_click(clicks):
        async for url, count in clicks.items():  # key, value
            counts[url] += int(count)

The data sent to a Kafka topic is partitioned, and since we use the URL
as a key in the "clicks" topic, that is how Kafka will shard the data
in such a way that every count from the same URL delivers to the
same Faust worker instance.

The state stored in tables may also be "windowed" so you can keep track
of "number of clicks from the last day," or
"number of clicks in the last hour.". We support tumbling, hopping
and sliding windows of time, and old windows can be expired to stop
data from filling up.

The data found in streams and tables can be anything: we support byte streams,
Unicode, and manually deserialized data structures. Taking this
further we have "Models" that use modern Python syntax to describe how
keys and values are serialized and deserialized:

.. sourcecode:: python

    class Order(faust.Record):
        account_id: str
        product_id: str
        price: float
        amount: float = 1.0

    orders_topic = app.topic('orders', key_type=str, value_type=Order)

    @app.agent(orders_topic)
    async def process_order(orders):
        async for order in orders:
            total_price = order.price * order.amount
            await send_order_received_email(order.account_id, order)

Faust is statically typed, using the :pypi:`mypy` type checker,
so you can take advantage of static types when writing applications.

**Learn more about Faust in the** :ref:`introduction` **introduction page**
    to read more about Faust, system requirements, installation instructions,
    community resources, and more.

**or go directly to the** :ref:`quickstart` **tutorial**
    to see Faust in action by programming a streaming application.

**then explore the** :ref:`User Guide <guide>`
    for in-depth information organized by topic.

.. _`Robinhood`: http://robinhood.com
.. _`async/await`:
    https://medium.freecodecamp.org/a-guide-to-asynchronous-programming-in-python-with-asyncio-232e2afa44f6
.. _`Celery`: http://celeryproject.org
.. _`Kafka Streams`: https://kafka.apache.org/documentation/streams
.. _`Apache Spark`: http://spark.apache.org
.. _`Storm`: http://storm.apache.org
.. _`Samza`: http://samza.apache.org
.. _`Flink`: http://flink.apache.org
.. _`RocksDB`: http://rocksdb.org
.. _`Apache Kafka`: https://kafka.apache.org
